{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트랜스포머야!\n",
    "- 트랜스포머지.\n",
    "- 일단 모델 설계부터 먼저 해볼까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 대충 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 'unsupervised' 데이터셋 제거\n",
    "del dataset['unsupervised']\n",
    "\n",
    "# 'train'과 'test' 데이터셋 크기 조정\n",
    "dataset['train'] = dataset['train'].select(range(10000))\n",
    "dataset['test'] = dataset['test'].select(range(10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    # 특정 필드만 float로 변환 (예: 'input_ids')\n",
    "    if 'input_ids' in tokenized:\n",
    "        tokenized['input_ids'] = [torch.tensor(ids, dtype=torch.float) for ids in tokenized['input_ids']]\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].remove_columns([\"text\"])\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format(\"torch\")\n",
    "\n",
    "test_dataset = tokenized_datasets[\"test\"].remove_columns([\"text\"])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일단 포지셔널 인코딩부터 만들자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.positional_encoding = self.get_pos_encoding(max_seq_len, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = position / torch.pow(10000, (2 * (i // 2)) / d_model)\n",
    "        return angles\n",
    "\n",
    "    def get_pos_encoding(self, max_seq_len, d_model):\n",
    "        angle_tensor = self.get_angles(\n",
    "            position=torch.arange(max_seq_len).unsqueeze(1),\n",
    "            i=torch.arange(d_model).unsqueeze(0),\n",
    "            d_model=torch.tensor(d_model, dtype=torch.float32)\n",
    "        )\n",
    "        positional_encoding = torch.zeros(max_seq_len, d_model, dtype=torch.float32)\n",
    "        positional_encoding[:, 0::2] = torch.sin(angle_tensor[:, 0::2])\n",
    "        positional_encoding[:, 1::2] = torch.cos(angle_tensor[:, 1::2])\n",
    "        return positional_encoding\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.positional_encoding[:seq_len, :].unsqueeze(0).to(x.device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "### pe 실험\n",
    "max_seq_len = 512\n",
    "d_model = 512\n",
    "\n",
    "x = torch.linspace(0, 1, max_seq_len).unsqueeze(0)\n",
    "pe =  PositionalEncoding(max_seq_len=max_seq_len, d_model=d_model)\n",
    "encoded_x = pe(x)\n",
    "\n",
    "# encoded_x =  x +  pe[:x.size(0), :]\n",
    "print(encoded_x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, use_encoder_output = False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.use_encoder_output = use_encoder_output\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def forward(self, x, mask, encoder_x = None):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        q = self.W_q(x)\n",
    "        if self.use_encoder_output:\n",
    "            k = self.W_k(encoder_x)\n",
    "            v = self.W_v(encoder_x)\n",
    "        else:\n",
    "            k = self.W_k(x)\n",
    "            v = self.W_v(x)\n",
    "        \n",
    "        # Split heads\n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        ## 이때 각각의 차원은 (batch_size, num_heads, seq_length, d_k)\n",
    "\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "\n",
    "        ## k.transpose 의 차원 (batch_size, num_heads, d_k, seq_length)\n",
    "        ## attention_scores의 차원 (batch_size, num_heads, seq_length, seq_length)\n",
    "        ## mask의 차원은 (batch_size, 1, seq_length, seq_length)\n",
    "        if mask is not None:\n",
    "            attention_scores += mask * (-1e9)\n",
    "        \n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        ### 이러면 (batch_size, num_heads, seq_length, seq_length)가 되고\n",
    "        context = torch.matmul(attention_probs, v)\n",
    "        ### 여기서 (batch_size, num_heads, seq_length, d_k)가 된다.\n",
    "        \n",
    "        # Combine heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        ## 여기서 x와 같은 차원(batch_size, seq_length, d_model)이 된다.\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        # return output\n",
    "        return self.layer_norm(x + output) ## ResidualConnection을 위해 더해주기, # LayerNormalization 해주기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear1(x)\n",
    "        output = self.relu(x)\n",
    "        output = self.linear2(x)\n",
    "\n",
    "        return self.layer_norm(x + output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, d_model=d_model)\n",
    "        self.ffnn = FFNN(d_model=d_model, d_ff=d_ff)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.att(x, mask)\n",
    "        x = self.ffnn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, num_heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.att1 = MultiHeadAttention(num_heads=num_heads, d_model=d_model)\n",
    "        self.att2 = MultiHeadAttention(num_heads=num_heads, d_model=d_model, use_encoder_output=True)\n",
    "        self.ffnn = FFNN(d_model=d_model, d_ff=d_ff)\n",
    "\n",
    "    def forward(self, x, mask, encoded_x):\n",
    "        if mask is None:\n",
    "            n = x.size(1)\n",
    "            look_ahead_mask = torch.triu(torch.ones(n, n), diagonal=1)  \n",
    "        else:\n",
    "            look_ahead_mask = torch.logical_or(mask, torch.triu(torch.ones(n, n), diagonal=1))\n",
    "        x = self.att1(x, look_ahead_mask)\n",
    "        x = self.att2(x, mask, encoded_x = encoded_x)\n",
    "        x = self.ffnn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, d_ff, num_heads, num_layers=6):\n",
    "        super(MyTransformer, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_ff, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_ff, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pe = PositionalEncoding(seq_len=seq_len, d_model=d_model)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src = self.pe(src)\n",
    "        tgt = self.pe(tgt)\n",
    "\n",
    "        # Encoder\n",
    "        encoded_src = src\n",
    "        for layer in self.encoder_layers:\n",
    "            encoded_src = layer(encoded_src, src_mask)\n",
    "\n",
    "        # Decoder\n",
    "        for layer in self.decoder_layers:\n",
    "            tgt = layer(tgt, tgt_mask, encoded_x=encoded_src)\n",
    "        \n",
    "        return tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "secondllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
